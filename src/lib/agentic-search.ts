/**
 * Agentic Search Engine
 * Superior to RAG through active planning, multi-source search, and continuous learning
 */

import { ModelConfig, ModelConfigManager, ModelProvider } from "./model-config";
import type { SearchResult } from "./types";

export interface SearchIntent {
	type: "factual" | "research" | "comparison" | "tutorial" | "news" | "analysis";
	complexity: "simple" | "moderate" | "complex";
	timeframe?: "recent" | "historical" | "current";
	sources: ("web" | "academic" | "news" | "documentation" | "social")[];
}

export interface SearchStrategy {
	primaryQuery: string;
	followUpQueries: string[];
	sources: string[];
	searchDepth: number;
	qualityThreshold: number;
}

export interface QualityAssessment {
	addScore: number; // Adversarial Differential Discrimination score
	credibility: number;
	freshness: number;
	relevance: number;
	diversity: number;
	flags: string[]; // adversarial content flags
}

export class AgenticSearchEngine {
	private modelManager: ModelConfigManager;

	constructor() {
		this.modelManager = new ModelConfigManager();
	}

	/**
	 * Main search method - superior to RAG through active reasoning
	 */
	async search(query: string, modelConfig?: ModelConfig): Promise<{
		results: SearchResult[];
		reasoning: string[];
		strategy: SearchStrategy;
		quality: QualityAssessment[];
	}> {
		const model = modelConfig || this.modelManager.getActiveConfig();
		if (!model) {
			throw new Error("No model configuration available");
		}

		const reasoning: string[] = [];

		// Phase 1: Intent Analysis
		reasoning.push("ðŸ” Analyzing search intent and query complexity...");
		const intent = await this.analyzeIntent(query, model);
		reasoning.push(`ðŸ“‹ Identified intent: ${intent.type} query with ${intent.complexity} complexity`);

		// Phase 2: Strategy Planning
		reasoning.push("ðŸŽ¯ Planning optimal search strategy...");
		const strategy = await this.planSearchStrategy(intent, model);
		reasoning.push(`ðŸ“Š Strategy: ${strategy.followUpQueries.length + 1} queries across ${strategy.sources.length} sources`);

		// Phase 3: Multi-Source Execution
		reasoning.push("ðŸŒ Executing multi-source search...");
		const rawResults = await this.executeMultiSourceSearch(strategy);
		reasoning.push(`ðŸ“ˆ Found ${rawResults.length} raw results from ${strategy.sources.length} sources`);

		// Phase 4: Quality Assessment (ADD Scoring)
		reasoning.push("âš–ï¸ Applying ADD quality assessment and ranking...");
		const { results, quality } = await this.assessAndRankResults(rawResults, strategy);
		reasoning.push(`âœ… Filtered to ${results.length} high-quality results (avg ADD: ${(quality.reduce((sum, q) => sum + q.addScore, 0) / quality.length).toFixed(2)})`);

		// Phase 5: Synthesis and Correlation
		reasoning.push("ðŸ§  Synthesizing findings and identifying correlations...");
		const synthesizedResults = await this.synthesizeResults(results, quality, model);
		reasoning.push("ðŸŽ‰ Synthesis complete - results ready for user");

		// Phase 6: Learning Update (continuous improvement)
		await this.updateLearningModel(query, intent, strategy, results);

		return {
			results: synthesizedResults,
			reasoning,
			strategy,
			quality,
		};
	}

	/**
	 * Phase 1: Intent Analysis - What does the user really want?
	 */
	private async analyzeIntent(query: string, model: ModelConfig): Promise<SearchIntent> {
		const prompt = `Analyze this search query and determine the user's intent:

Query: "${query}"

Return a JSON object with:
{
  "type": "factual|research|comparison|tutorial|news|analysis",
  "complexity": "simple|moderate|complex",
  "timeframe": "recent|historical|current" (optional),
  "sources": ["web", "academic", "news", "documentation", "social"] (prioritized)
}

Be specific about the intent type and appropriate sources.`;

		const response = await this.callModel(prompt, model);
		try {
			const parsed = JSON.parse(response);
			return {
				type: parsed.type || "factual",
				complexity: parsed.complexity || "moderate",
				timeframe: parsed.timeframe,
				sources: parsed.sources || ["web"],
			};
		} catch {
			// Fallback intent analysis
			return {
				type: this.detectIntentType(query),
				complexity: this.detectComplexity(query),
				sources: ["web", "academic"],
			};
		}
	}

	/**
	 * Phase 2: Strategy Planning - How to search effectively?
	 */
	private async planSearchStrategy(intent: SearchIntent, model: ModelConfig): Promise<SearchStrategy> {
		const prompt = `Create an optimal search strategy for this intent:

Intent: ${JSON.stringify(intent, null, 2)}

Generate a search strategy with:
{
  "primaryQuery": "optimized main search query",
  "followUpQueries": ["refinement query 1", "refinement query 2"],
  "sources": ["source1", "source2"],
  "searchDepth": 2-5,
  "qualityThreshold": 0.7-0.9
}

Make queries specific and effective for the intent type.`;

		const response = await this.callModel(prompt, model);
		try {
			const parsed = JSON.parse(response);
			return {
				primaryQuery: parsed.primaryQuery || intent.type,
				followUpQueries: parsed.followUpQueries || [],
				sources: parsed.sources || intent.sources,
				searchDepth: parsed.searchDepth || 3,
				qualityThreshold: parsed.qualityThreshold || 0.8,
			};
		} catch {
			// Fallback strategy
			return {
				primaryQuery: intent.type,
				followUpQueries: [],
				sources: intent.sources,
				searchDepth: 3,
				qualityThreshold: 0.8,
			};
		}
	}

	/**
	 * Phase 3: Multi-Source Search Execution
	 */
	private async executeMultiSourceSearch(strategy: SearchStrategy): Promise<any[]> {
		const results: any[] = [];

		// Execute primary query
		const primaryResults = await this.searchSource(strategy.primaryQuery, strategy.sources[0]);
		results.push(...primaryResults);

		// Execute follow-up queries for deeper search
		for (const followUpQuery of strategy.followUpQueries) {
			for (const source of strategy.sources.slice(1)) {
				const followUpResults = await this.searchSource(followUpQuery, source);
				results.push(...followUpResults);
			}
		}

		return results;
	}

	/**
	 * Search individual source using real APIs (Firecrawl for web)
	 */
	private async searchSource(query: string, source: string): Promise<any[]> {
		// Check if Firecrawl API is available
		const firecrawlApiKey = import.meta.env.VITE_FIRECRAWL_API_KEY || process.env.FIRECRAWL_API_KEY;

		if (firecrawlApiKey && source === 'web') {
			try {
				// Use real Firecrawl search
				const { FirecrawlClient } = await import('./firecrawl-client');
				const firecrawl = new FirecrawlClient({ apiKey: firecrawlApiKey });

				const results = await firecrawl.search({
					query,
					limit: 5,
					formats: ['markdown'],
					onlyMainContent: true,
				});

				console.log(`[AgenticSearch] Firecrawl returned ${results.length} results for: ${query}`);

				return results.map((result, index) => ({
					id: `firecrawl-${Date.now()}-${index}`,
					title: result.title,
					snippet: result.description || result.markdown?.substring(0, 200) || '',
					url: result.url,
					source: 'web',
					publishedDate: result.metadata?.publishedDate || new Date().toISOString(),
					rawScore: 0.8 + (Math.random() * 0.2), // 0.8-1.0 for Firecrawl results
					markdown: result.markdown, // Full content for synthesis
				}));
			} catch (error) {
				console.error('[AgenticSearch] Firecrawl error:', error);
				// Fall through to mock data
			}
		}

		// Fallback to mock data when Firecrawl unavailable or for other sources
		console.log(`[AgenticSearch] Using mock data for source: ${source}`);

		await new Promise(resolve => setTimeout(resolve, 200)); // Simulate API delay

		return [
			{
				id: `${source}-${Date.now()}-${Math.random()}`,
				title: `Search Result for: ${query}`,
				snippet: `Comprehensive information about ${query} from ${source} source.`,
				url: `https://${source}.example.com/result`,
				source,
				publishedDate: new Date().toISOString(),
				rawScore: Math.random() * 0.5 + 0.5, // 0.5-1.0
			},
			{
				id: `${source}-${Date.now()}-${Math.random()}`,
				title: `Additional findings: ${query}`,
				snippet: `Further details and analysis related to ${query}.`,
				url: `https://${source}.example.com/additional`,
				source,
				publishedDate: new Date(Date.now() - 86400000).toISOString(),
				rawScore: Math.random() * 0.3 + 0.4, // 0.4-0.7
			}
		];
	}

	/**
	 * Phase 4: Quality Assessment with ADD Scoring
	 */
	private async assessAndRankResults(
		rawResults: any[],
		strategy: SearchStrategy
	): Promise<{ results: SearchResult[]; quality: QualityAssessment[] }> {
		const assessments: QualityAssessment[] = [];
		const filteredResults: SearchResult[] = [];

		for (const result of rawResults) {
			const assessment = await this.assessQuality(result, strategy);
			assessments.push(assessment);

			if (assessment.addScore >= strategy.qualityThreshold) {
				filteredResults.push({
					id: result.id,
					title: result.title,
					snippet: result.snippet,
					url: result.url,
					source: result.source as any,
					addScore: assessment.addScore,
					publishedDate: result.publishedDate,
				});
			}
		}

		// Sort by ADD score descending
		filteredResults.sort((a, b) => (b.addScore || 0) - (a.addScore || 0));

		return { results: filteredResults, quality: assessments };
	}

	/**
	 * Assess individual result quality using ADD scoring
	 *
	 * TODO [TRACK-001]: Replace deterministic scoring with ML model integration
	 * - Integrate real ADD (Adversarial Differential Discrimination) model
	 * - Add feature flag: ENABLE_ML_SCORING
	 * - Add input validation for result.rawScore and strategy
	 * - Surface errors via proper logging infrastructure
	 * - Add unit tests with mocked ML service
	 */
	private async assessQuality(result: any, strategy: SearchStrategy): Promise<QualityAssessment> {
		// Validate inputs
		if (!result || typeof result !== 'object') {
			console.error('[assessQuality] Invalid result object:', result);
			return this.getFallbackAssessment();
		}

		if (!strategy || typeof strategy !== 'object') {
			console.error('[assessQuality] Invalid strategy object:', strategy);
			return this.getFallbackAssessment();
		}

		const baseScore = typeof result.rawScore === 'number'
			? Math.max(0, Math.min(1, result.rawScore))
			: 0.5;

		// Deterministic scoring based on result properties (testable, not random)
		// This provides consistent scores for the same inputs
		const contentLength = result.snippet?.length || 0;
		const hasMetadata = Boolean(result.metadata && Object.keys(result.metadata).length > 0);
		const hasPublishDate = Boolean(result.publishedDate);
		const sourceQuality = this.getSourceQualityScore(result.source);

		// Calculate deterministic ADD score components
		const credibility = Math.min(1.0, sourceQuality * 0.4 + (hasMetadata ? 0.3 : 0) + 0.3);
		const freshness = hasPublishDate ? this.calculateFreshnessScore(result.publishedDate) : 0.5;
		const relevance = Math.min(1.0, baseScore * 0.7 + (contentLength > 100 ? 0.3 : 0.1));
		const diversity = Math.min(1.0, 0.6 + (hasMetadata ? 0.2 : 0) + (contentLength > 200 ? 0.2 : 0));

		const addScore = Math.min(1.0, (credibility + freshness + relevance + diversity) / 4);

		return {
			addScore,
			credibility,
			freshness,
			relevance,
			diversity,
			flags: this.detectQualityFlags(result),
		};
	}

	/**
	 * Get deterministic source quality score
	 */
	private getSourceQualityScore(source: string): number {
		// Deterministic scoring based on known source types
		const qualityMap: Record<string, number> = {
			'firecrawl': 0.8,
			'autumn': 0.7,
			'direct': 0.9,
		};
		return qualityMap[source?.toLowerCase()] || 0.6;
	}

	/**
	 * Calculate freshness score based on publish date
	 */
	private calculateFreshnessScore(publishedDate: string | number): number {
		try {
			const date = new Date(publishedDate);
			const now = Date.now();
			const ageInDays = (now - date.getTime()) / (1000 * 60 * 60 * 24);

			// Score decreases with age: fresh content scores higher
			if (ageInDays < 7) return 1.0;
			if (ageInDays < 30) return 0.9;
			if (ageInDays < 90) return 0.8;
			if (ageInDays < 365) return 0.7;
			return 0.6;
		} catch (error) {
			console.error('[calculateFreshnessScore] Invalid date:', publishedDate);
			return 0.5;
		}
	}

	/**
	 * Detect quality flags in result
	 */
	private detectQualityFlags(result: any): string[] {
		const flags: string[] = [];

		if (!result.snippet || result.snippet.length < 50) {
			flags.push('low-content');
		}

		if (!result.publishedDate) {
			flags.push('no-date');
		}

		if (!result.source) {
			flags.push('unknown-source');
		}

		return flags;
	}

	/**
	 * Fallback assessment for error cases
	 */
	private getFallbackAssessment(): QualityAssessment {
		return {
			addScore: 0.5,
			credibility: 0.5,
			freshness: 0.5,
			relevance: 0.5,
			diversity: 0.5,
			flags: ['error-fallback'],
		};
	}

	/**
	 * Phase 5: Synthesis and Correlation
	 */
	private async synthesizeResults(
		results: SearchResult[],
		quality: QualityAssessment[],
		model: ModelConfig
	): Promise<SearchResult[]> {
		if (results.length <= 3) return results;

		// Use AI to identify correlations and remove duplicates
		const synthesisPrompt = `Synthesize these search results, identify correlations, and remove duplicates:

Results: ${JSON.stringify(results.slice(0, 10), null, 2)}

Return the top 5 most relevant, non-duplicate results with improved titles and snippets.`;

		try {
			const response = await this.callModel(synthesisPrompt, model);
			const synthesized = JSON.parse(response);

			// Verify the response is an array before using it
			if (Array.isArray(synthesized)) {
				return synthesized.slice(0, 5);
			} else {
				console.warn('[synthesizeResults] AI response was not an array:', typeof synthesized);
				return results.slice(0, 5);
			}
		} catch (error) {
			console.error('[synthesizeResults] Failed to parse or synthesize results:', error);
			// Fallback: return top results
			return results.slice(0, 5);
		}
	}

	/**
	 * Phase 6: Continuous Learning
	 */
	private async updateLearningModel(
		query: string,
		intent: SearchIntent,
		strategy: SearchStrategy,
		results: SearchResult[]
	): Promise<void> {
		// Store successful search patterns for future optimization
		// This would update a learning model that improves search strategies over time

		// Guard against division by zero
		const resultCount = results.length;
		const avgScore = resultCount > 0
			? results.reduce((sum, r) => sum + (r.addScore || 0), 0) / resultCount
			: 0;

		// Ensure avgScore is finite
		const validAvgScore = Number.isFinite(avgScore) ? avgScore : 0;

		const learningData = {
			query,
			intent,
			strategy,
			resultCount,
			avgScore: validAvgScore,
			timestamp: Date.now(),
		};

		// In production, this would update a database or ML model
		console.log("Learning update:", learningData);
	}

	/**
	 * Call the configured AI model
	 */
	private async callModel(prompt: string, model: ModelConfig): Promise<string> {
		switch (model.provider) {
			case ModelProvider.OLLAMA:
				return this.callOllama(prompt, model);
			case ModelProvider.ANTHROPIC:
				return this.callAnthropic(prompt, model);
			case ModelProvider.OPENAI:
				return this.callOpenAI(prompt, model);
			default:
				throw new Error(`Unsupported provider: ${model.provider}`);
		}
	}

	/**
	 * Call Ollama model
	 */
	private async callOllama(prompt: string, model: ModelConfig): Promise<string> {
		try {
			const response = await fetch(`${model.baseUrl}/api/generate`, {
				method: "POST",
				headers: { "Content-Type": "application/json" },
				body: JSON.stringify({
					model: model.model,
					prompt,
					stream: false,
					options: {
						temperature: model.temperature,
						num_predict: model.maxTokens,
					},
				}),
			});

			if (!response.ok) {
				throw new Error(`Ollama API error: ${response.status}`);
			}

			const data = await response.json();
			return data.response || "";
		} catch (error) {
			console.error("Ollama call failed:", error);
			throw new Error(`Failed to call Ollama: ${error}`);
		}
	}

	/**
	 * Call Anthropic API
	 */
	private async callAnthropic(prompt: string, model: ModelConfig): Promise<string> {
		try {
			const response = await fetch(`${model.baseUrl}/v1/messages`, {
				method: "POST",
				headers: {
					"Content-Type": "application/json",
					"x-api-key": model.apiKey || "",
					"anthropic-version": "2023-06-01",
				},
				body: JSON.stringify({
					model: model.model,
					max_tokens: model.maxTokens,
					temperature: model.temperature,
					messages: [{ role: "user", content: prompt }],
				}),
			});

			if (!response.ok) {
				throw new Error(`Anthropic API error: ${response.status}`);
			}

			const data = await response.json();
			return data.content?.[0]?.text || "";
		} catch (error) {
			console.error("Anthropic call failed:", error);
			throw new Error(`Failed to call Anthropic: ${error}`);
		}
	}

	/**
	 * Call OpenAI API
	 */
	private async callOpenAI(prompt: string, model: ModelConfig): Promise<string> {
		try {
			const response = await fetch(`${model.baseUrl}/v1/chat/completions`, {
				method: "POST",
				headers: {
					"Content-Type": "application/json",
					"Authorization": `Bearer ${model.apiKey}`,
				},
				body: JSON.stringify({
					model: model.model,
					messages: [{ role: "user", content: prompt }],
					temperature: model.temperature,
					max_tokens: model.maxTokens,
				}),
			});

			if (!response.ok) {
				throw new Error(`OpenAI API error: ${response.status}`);
			}

			const data = await response.json();
			return data.choices?.[0]?.message?.content || "";
		} catch (error) {
			console.error("OpenAI call failed:", error);
			throw new Error(`Failed to call OpenAI: ${error}`);
		}
	}

	/**
	 * Fallback intent detection
	 */
	private detectIntentType(query: string): SearchIntent["type"] {
		const lower = query.toLowerCase();
		if (lower.includes("how to") || lower.includes("tutorial")) return "tutorial";
		if (lower.includes("compare") || lower.includes("vs")) return "comparison";
		if (lower.includes("research") || lower.includes("study")) return "research";
		if (lower.includes("news") || lower.includes("latest")) return "news";
		if (lower.includes("analyze") || lower.includes("explain")) return "analysis";
		return "factual";
	}

	/**
	 * Fallback complexity detection
	 */
	private detectComplexity(query: string): SearchIntent["complexity"] {
		const words = query.split(" ").length;
		if (words < 5) return "simple";
		if (words < 15) return "moderate";
		return "complex";
	}
}

// Singleton instance
export const agenticSearch = new AgenticSearchEngine();
